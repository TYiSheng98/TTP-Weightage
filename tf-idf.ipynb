{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_root = os.getcwd()\n",
    "dataset_folderpath= os.path.join(prj_root,'mitre_nav_reports')\n",
    "ttp_list_corpus=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-farming",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(dataset_folderpath):\n",
    "    for filename in files:\n",
    "        abs_filepath = os.path.join(root,filename)\n",
    "        # print(abs_filepath)\n",
    "        with open(abs_filepath,'r') as f:\n",
    "            data = data = json.load(f)\n",
    "\n",
    "        df = pd.DataFrame(data['techniques'])\n",
    "        # df = df[df['score'].notna()]\n",
    "        df = df.loc[df['score'].isin([1.0])]\n",
    "        ttp_list= df['techniqueID'].tolist()\n",
    "        for ttp in ttp_list:\n",
    "            ttp_list_corpus.append(ttp)\n",
    "        \n",
    "        # ttp_list_corpus.append(' '.join(ttp_list))\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src:https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer(analyzer=lambda d: d.split(' ')) \n",
    "word_count_vector=cv.fit_transform(ttp_list_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-cradle",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#freq_count of all ttps\n",
    "freq_count_dict = dict(zip(cv.get_feature_names(),word_count_vector.toarray().sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-psychiatry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "# tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-hearts",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you only needs to do this once\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([' '.join(ttp_list_corpus)]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,len(feature_names))\n",
    "\n",
    "\n",
    "# for k in keywords:\n",
    "#     print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-coating",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([keywords])\n",
    "df1 = df1.T\n",
    "df1.index.name = 'TTP'\n",
    "df1.columns = ['weight']\n",
    "df1\n",
    "# df1.to_csv (r'tf-tdf.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-catholic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttp_list_corpus.sort()\n",
    "len(ttp_list_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-automation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(ttp_list_corpus,\n",
    "               columns =['TTP'])\n",
    "s = df3.value_counts(normalize=True).sort_index()\n",
    "pd11 = pd.DataFrame(s).reset_index()\n",
    "pd11.columns = ['TTP', 'weight']\n",
    "# s= s.sort_values(ascending=False)\n",
    "pd11.sort_values(by=['weight'],ascending=False, inplace=True)\n",
    "pd11\n",
    "# pd11.to_csv('freq_percentage.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-aberdeen",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df4 = df3.groupby(by=[\"TTP\"]).size().reset_index(name='counts')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-collector",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# src:https://www.geeksforgeeks.org/data-normalization-with-pandas/\n",
    "# Using The maximum absolute scaling\n",
    "# copy the data\n",
    "df_max_scaled = df4.copy()\n",
    "# df_max_scaled = df_max_scaled.drop(columns=['TTP'])\n",
    "df_max_scaled\n",
    "# apply normalization techniques\n",
    "\n",
    "df_max_scaled['weight'] = df_max_scaled['counts']  / df_max_scaled['counts'] .abs().max()\n",
    "      \n",
    "# view normalized data\n",
    "df_max_scaled= df_max_scaled.sort_values('weight',ascending=False)\n",
    "# df_max_scaled.to_csv('maximum_absolute_scaling.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using The min-max feature scaling\n",
    "# copy the data\n",
    "df_min_max_scaled = df4.copy()\n",
    "  \n",
    "# apply normalization techniques\n",
    "df_min_max_scaled['NORM'] = ((df_min_max_scaled['counts']- df_min_max_scaled['counts'].min()) / (df_min_max_scaled['counts'].max() - df_min_max_scaled['counts'].min()))   \n",
    "  \n",
    "# view normalized data\n",
    "df_min_max_scaled=df_min_max_scaled.sort_values('NORM',ascending=False)\n",
    "# df_min_max_scaled.to_csv(' min_max_scaling.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-century",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttp_list_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1a78f-536d-4665-9381-e6d6de0e11b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
